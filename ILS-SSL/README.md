
# ILS-SSL

> [**WavLM**](https://arxiv.org/pdf/2112.08778.pdf): Self-Supervised Learning for Speech Recognition with Intermediate Layer Supervision

The data preparation and pre-training for the first iteration follow the same pipeline as Hubert. We give example scripts for ILS-Hubert pre-training and fine-tuning in src/examples/hubert/scripts

## Pre-Trained and Fine-tuned Models
Model | Pretraining Dataset | Finetuning Dataset | Model
|---|---|---|---
ILS-Base | 960h LibriSpeech | - |comming soon
ILS-Large | 60k hrs Libri-Light | - |comming soon



